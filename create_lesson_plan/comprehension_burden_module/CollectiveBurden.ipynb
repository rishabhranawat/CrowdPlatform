{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Collective Burden for Sequencing Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from collections import Counter\n",
    "import random as randomlib\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "import itertools\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Knowledge Graph\n",
    "'''\n",
    "kg_path = \"../graph_query/graphs/weighted_knowledge_graph.gpickle\"\n",
    "kg = nx.read_gpickle(kg_path)\n",
    "kg_labels = [str(x) for x in list(kg.nodes())[1:]]\n",
    "n_labels = len(kg_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Get content from a given set of URLs.\n",
    "'''\n",
    "def get_content(url):\n",
    "    es_order = []\n",
    "    f = open(url, 'r')\n",
    "    l = f.readlines()\n",
    "    docs = {}\n",
    "    index = {}\n",
    "    \n",
    "    counter = 0\n",
    "    for url in l:\n",
    "        try:\n",
    "            docs[url] = requests.get(url).content\n",
    "            index[url] = counter\n",
    "            es_order.append(url)\n",
    "            counter += 1\n",
    "        except:\n",
    "            continue\n",
    "    return docs, index, es_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Term Frequency Array for a particular document.\n",
    "'''\n",
    "def get_tfd(content):\n",
    "    word_count_dict = Counter(w for w in kg_labels \n",
    "                              if w.lower() in content.lower())\n",
    "    common = word_count_dict.most_common()\n",
    "    \n",
    "    frequency_arr = [0]*len(kg_labels)\n",
    "    \n",
    "    for common_word in common:\n",
    "        common_word_index = kg_labels.index(common_word[0])\n",
    "        frequency_arr[common_word_index] = common_word[1]\n",
    "    return frequency_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Building word_data a document (rows) by term frequency (columns) matrix.\n",
    "'''\n",
    "def get_matrices(content, index):\n",
    "    tfd_data = {}\n",
    "    for url, cont in content.items():\n",
    "        tfd_data[url] = get_tfd(cont)\n",
    "\n",
    "    tfd_arr = []\n",
    "    for key in index.keys():\n",
    "        tfd_arr.append(key.replace(\"\\n\", \"\"))\n",
    "\n",
    "    word_data = {'TFD':tfd_arr}\n",
    "\n",
    "    for label in kg_labels:\n",
    "        word_data[label] = [None]*len(index)\n",
    "\n",
    "    for url, words_in_doc in tfd_data.items():\n",
    "        url_index = index[url]\n",
    "        for i in range(0, n_labels, 1):\n",
    "            word = kg_labels[i]\n",
    "            word_data[word][url_index] = words_in_doc[i]\n",
    "\n",
    "    '''\n",
    "    (DTF)^T(DTF) = Coocurence Matrix\n",
    "    '''\n",
    "    document_term_frequency = pd.DataFrame(word_data).set_index('TFD')\n",
    "    dtf_asint = document_term_frequency.astype(int)\n",
    "    coocc = dtf_asint.T.dot(dtf_asint)\n",
    "\n",
    "    return document_term_frequency, dtf_asint, coocc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Calculating Relationship Score: S(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_relationship_between_concepts(concept_1, concept_2, document_term_frequency):\n",
    "    concept_1_index= document_term_frequency.columns.get_loc(concept_1)\n",
    "    concept_2_index= document_term_frequency.columns.get_loc(concept_2)\n",
    "    \n",
    "    return coocc.iloc[concept_1_index, concept_2_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Significance of a concept in a document: \\lambda(c, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_significance_score(concept, index, document, document_term_frequency, dtf_asint, coocc):\n",
    "    if(document == None): return 0\n",
    "    concept_index = document_term_frequency.columns.get_loc(concept)\n",
    "    freq = dtf_asint.iloc[index[document]][concept_index]\n",
    "    coocc_row = coocc.iloc[concept_index,:] \n",
    "    r = np.array(coocc_row)\n",
    "    if(sum(r) == 0): return freq\n",
    "    return (freq)+np.count_nonzero(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_right(content, index, top_n, document_term_frequency, dtf_asint, coocc):\n",
    "    doc_to_concepts_list = {}\n",
    "    for each_document in content.keys():\n",
    "        doc_to_concepts_list[each_document] = []\n",
    "    for each_concept in kg_labels:\n",
    "        m = 0.0\n",
    "        d_to_v = {}\n",
    "        for each_document in content.keys():\n",
    "            d_to_v[each_document] = get_significance_score(each_concept, index, each_document, document_term_frequency,\n",
    "                                                          dtf_asint, coocc)\n",
    "            if(d_to_v[each_document] > m):\n",
    "                m = d_to_v[each_document]\n",
    "        \n",
    "        for d, v in d_to_v.items():\n",
    "            if(v == m):\n",
    "                doc_to_concepts_list[d].append((each_concept, v))\n",
    "    \n",
    "    final_doc_to_concept_list = {}\n",
    "    for d, v in doc_to_concepts_list.items():\n",
    "        v.sort(key=lambda x:x[1])\n",
    "        if(len(v) >= top_n):\n",
    "            final_doc_to_concept_list[d] = [v[i][0] for i in range(0, top_n, 1)]\n",
    "        else:\n",
    "            final_doc_to_concept_list[d] = [x[0] for x in v]\n",
    "    relevant_concepts= set()\n",
    "    \n",
    "    for d, v in final_doc_to_concept_list.items():\n",
    "        for each in v:\n",
    "            relevant_concepts.add(each[0])\n",
    "    return doc_to_concepts_list, relevant_concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Key Sections k_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_doc_to_concepts_list(content, index, top_n, document_term_frequency, dtf_asint, coocc):\n",
    "    doc_to_concept_list = {}\n",
    "    relevant_concepts_to_sequence = set()\n",
    "    \n",
    "    for each_document in content.keys():\n",
    "        rt = []\n",
    "        rc = []\n",
    "        for each_concept in kg_labels:\n",
    "            s = get_significance_score(each_concept, index,each_document, document_term_frequency, dtf_asint, coocc)\n",
    "            if(s <= 0): continue\n",
    "            if(\"NodeType\" not in kg.node[each_concept]):\n",
    "                continue\n",
    "            elif(kg.node[each_concept][\"NodeType\"] == \"ConceptNode\"):\n",
    "                rc.append((each_concept, s))\n",
    "            elif(kg.node[each_concept][\"NodeType\"] == \"TopicNode\"):\n",
    "                rt.append((each_concept, s))\n",
    "        \n",
    "        rt.sort(key=lambda x:x[1])\n",
    "        rt = rt[::-1]\n",
    "        \n",
    "        rc.sort(key=lambda x:x[1])\n",
    "        rc = rc[::-1]\n",
    "        \n",
    "        key_concepts = []\n",
    "        while(len(rc) and len(key_concepts) < top_n):\n",
    "            key_concepts.append(rc[0][0])\n",
    "            relevant_concepts_to_sequence.add(rc[0][0])\n",
    "            rc.pop(0)\n",
    "            \n",
    "        while(len(rt) and len(key_concepts) < top_n):\n",
    "            key_concepts.append(rt[0][0])\n",
    "            relevant_concepts_to_sequence.add(rt[0][0])\n",
    "            rt.pop(0)\n",
    "            \n",
    "        for each in rt:\n",
    "            relevant_concepts_to_sequence.add(each[0])\n",
    "        \n",
    "        for each in rc:\n",
    "            relevant_concepts_to_sequence.add(each[0])\n",
    "        doc_to_concept_list[each_document] = key_concepts\n",
    "    return doc_to_concept_list, relevant_concepts_to_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_relevant_concepts_for_lp(doc_to_concepts_list):\n",
    "    rel = []\n",
    "    for key,val in doc_to_concepts_list.items():\n",
    "        for each in val:\n",
    "            rel.append(each)\n",
    "    return rel   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Comprehension Burden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def f_cb(sig_score, key_sig_score, relationship):\n",
    "    return sig_score+key_sig_score\n",
    "\n",
    "def get_related_concepts(document, index, document_term_frequency):\n",
    "    concepts = []x\n",
    "    a = np.array(document_term_frequency.iloc[index[document]])\n",
    "    z = a.nonzero()\n",
    "    if(len(z[0]) == 0): return []\n",
    "    for x in np.nditer(z[0]):\n",
    "        concepts.append(document_term_frequency.columns[x])   \n",
    "    return concepts\n",
    "    \n",
    "def get_cb_document(document, dc, visited, relevant, \n",
    "                    document_term_frequency, dtf_asint, coocc, index):\n",
    "    document_burden = 0.0\n",
    "    ds = get_related_concepts(document, index, document_term_frequency)\n",
    "    for d in ds:\n",
    "        burden = 0.0\n",
    "        count = 0\n",
    "        for c in dc:\n",
    "            if(get_relationship_between_concepts(d, c, document_term_frequency) > 0):\n",
    "                count += 1\n",
    "                if(d not in visited):\n",
    "                    burden += get_significance_score(c, index, document, document_term_frequency, dtf_asint, coocc)\n",
    "                else:\n",
    "                    count += 1\n",
    "        if(count > 0):\n",
    "            document_burden += burden/count\n",
    "    return document_burden\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Sequence Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_linear(nodes):\n",
    "    parents = []\n",
    "    for each in nodes:\n",
    "        if(each in kg.nodes and kg.nodes[each][\"NodeType\"] == \"TopicNode\"):\n",
    "            parents.append(each)\n",
    "    \n",
    "    linear = []\n",
    "    for p in parents:\n",
    "        linear.append(p)\n",
    "        children = kg.neighbors(p)\n",
    "        for c in children:\n",
    "            if c in nodes and kg.nodes[c][\"NodeType\"] == \"ConceptNode\":\n",
    "                linear.append(c)\n",
    "    \n",
    "    for each in nodes:\n",
    "        if each not in linear:\n",
    "            linear.append(each)\n",
    "    \n",
    "    return linear\n",
    "\n",
    "def get_weighted_sequences(nodes):\n",
    "    parents = []\n",
    "    for each in nodes:\n",
    "        if(each in kg.nodes and kg.nodes[each][\"NodeType\"] == \"TopicNode\"):\n",
    "            parents.append(each)\n",
    "            \n",
    "    weighted = []\n",
    "    for p in parents:\n",
    "        weighted.append(p)\n",
    "        children = kg.neighbors(p)\n",
    "        all_c = []\n",
    "        \n",
    "        for c in children:\n",
    "            if(c not in nodes): continue\n",
    "            if(\"weight\" in kg[p][c]):\n",
    "                all_c.append((c, kg[p][c][\"weight\"]))\n",
    "            else:\n",
    "                all_c.append((c, 0.0))\n",
    "\n",
    "        all_c.sort(key=lambda x:x[1])\n",
    "        all_c = all_c[::-1]\n",
    "\n",
    "        for e in all_c: weighted.append(e[0])\n",
    "    return weighted\n",
    "            \n",
    "    \n",
    "def get_sequences(nodes):\n",
    "        linear = get_linear(nodes)\n",
    "        top_down = linear[::-1]\n",
    "        weighted = get_weighted_sequences(nodes)\n",
    "        return linear, top_down, weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_concepts_to_document_list(doc_to_concepts_list, relevant_concepts):\n",
    "    concept_to_document_list = {}\n",
    "    for each_concept in relevant_concepts:\n",
    "        concept_to_document_list[each_concept] = []\n",
    "        for doc, kcs in doc_to_concepts_list.items():\n",
    "            if(each_concept in kcs): \n",
    "                concept_to_document_list[each_concept].append(doc)\n",
    "    return concept_to_document_list\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_burden_for_a_sequence(docs_sequence, doc_to_concepts_list, concepts_to_document_list,\n",
    "                                   document_term_frequency, dft_asint, coocc, index, relevant_concepts_to_sequence):\n",
    "    visited = set()\n",
    "    collective_burden = 0.0\n",
    "    burdens = []\n",
    "    for each_doc in docs_sequence:\n",
    "        \n",
    "        for each_ass_conc in doc_to_concepts_list[each_doc]:\n",
    "            visited.add(each_ass_conc)\n",
    "\n",
    "        burden_per_doc = get_cb_document(each_doc, doc_to_concepts_list[each_doc], \n",
    "                                                     visited, relevant_concepts_to_sequence, \n",
    "                                                     document_term_frequency, dtf_asint, coocc, index)\n",
    "        collective_burden += burden_per_doc\n",
    "        burdens.append(burden_per_doc)\n",
    "        \n",
    "        \n",
    "    return collective_burden\n",
    "\n",
    "def get_burden_for_all_permutations(doc_to_concepts_list, concepts_to_document_list,\n",
    "                                   document_term_frequency, dft_asint, coocc, index, relevant_concepts_to_sequence):\n",
    "    \n",
    "    docs = [x for x in doc_to_concepts_list.iterkeys()]\n",
    "    perms = list(itertools.permutations(docs))\n",
    "    \n",
    "    for each in perms: print(get_burden_for_a_sequence(each, doc_to_concepts_list, concepts_to_document_list,\n",
    "                                   document_term_frequency, dft_asint, coocc, index, relevant_concepts_to_sequence))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_burden_for_sequence(sequence, doc_to_concepts_list, \n",
    "                            concepts_to_document_list, \n",
    "                            document_term_frequency, dtf_asint, coocc, index):\n",
    "    collective_burden = 0.0\n",
    "    docs_sequence = []\n",
    "    \n",
    "    for each_con in sequence:\n",
    "        docs_ass = concepts_to_document_list[each_con]\n",
    "        \n",
    "        doc_ass_size = []\n",
    "        for each in docs_ass:\n",
    "            doc_ass_size.append((each, \n",
    "                                 get_significance_score(each_con, index, each, document_term_frequency, dtf_asint, coocc)))\n",
    "        doc_ass_size.sort(key=lambda x:x[1])\n",
    "        doc_ass_size = doc_ass_size[::-1]\n",
    "        \n",
    "        for each, v in doc_ass_size:\n",
    "            if(each not in docs_sequence):\n",
    "                docs_sequence.append(each)\n",
    "\n",
    "    for each in doc_to_concepts_list.keys():\n",
    "        if(each not in docs_sequence): docs_sequence.append(each)\n",
    "    \n",
    "    visited = set()\n",
    "    burdens = []\n",
    "    for each_doc in docs_sequence:\n",
    "        for each_ass_conc in doc_to_concepts_list[each_doc]:\n",
    "            visited.add(each_ass_conc)\n",
    "        \n",
    "        burden_per_doc = get_cb_document(each_doc, doc_to_concepts_list[each_doc], \n",
    "                                                     visited, sequence, \n",
    "                                                     document_term_frequency, dtf_asint, coocc, index)\n",
    "        collective_burden += burden_per_doc\n",
    "        burdens.append(burden_per_doc)\n",
    "        \n",
    "        \n",
    "    return collective_burden, burdens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_score(content, index, top_n, document_term_frequency, dtf_asint, coocc):\n",
    "    doc_to_concepts_list, relevant_concepts_to_sequence = get_doc_to_concepts_list(content, index, top_n, document_term_frequency, dtf_asint, coocc)\n",
    "    concepts_to_document_list = get_concepts_to_document_list(doc_to_concepts_list, relevant_concepts_to_sequence)\n",
    "    \n",
    "    linear, bottom_up, weighted = get_sequences(relevant_concepts_to_sequence)\n",
    "    s, burden_per_doc = get_burden_for_sequence(linear, doc_to_concepts_list, concepts_to_document_list, \n",
    "                                document_term_frequency, dtf_asint, coocc, index)\n",
    "    print(\"len\", len(doc_to_concepts_list))\n",
    "    return (s, doc_to_concepts_list, max(burden_per_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_required(url):\n",
    "    content, index, es_order = get_content(url)\n",
    "    document_term_frequency, dtf_asint, coocc = get_matrices(content, index)\n",
    "    return content, document_term_frequency, dtf_asint, coocc, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "content, document_term_frequency, dtf_asint, coocc, index = get_required(\"lps/engage/user_study_graph_theory_engage.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('len', 32)\n",
      "('https://jeremykun.com/2013/01/22/depth-and-breadth-first-search/\\n', ['Dfs', 'Shortest Path', 'Randomized algorithms', 'Shortest Paths', 'Heaps', 'Scheduling', 'Topological Sort', 'Huffman codes', 'Strongly Connected Components', 'Longest common subsequence'])\n",
      "('https://blog.oureducation.in/depth-first-search-and-breadth-first-search-bfs-and-dfs/\\n', ['Dfs', 'Shortest Path', 'Randomized algorithms', 'Shortest Paths', 'Heaps', 'Scheduling', 'Topological Sort', 'Huffman codes', 'Strongly Connected Components', 'Longest common subsequence'])\n",
      "('https://www.codeproject.com/Messages/5044791/To-reduce-cost-of-adjMatrix.aspx\\n', ['Dfs', 'Shortest Path', 'Randomized algorithms', 'Shortest Paths', 'Heaps', 'Scheduling', 'Topological Sort', 'Huffman codes', 'Strongly Connected Components', 'Longest common subsequence'])\n",
      "('https://en.wikipedia.org/wiki/Expected_linear_time_MST_algorithm\\n', ['Dfs', 'Shortest Path', 'Randomized algorithms', 'Shortest Paths', 'Heaps', 'Scheduling', 'Topological Sort', 'Huffman codes', 'Strongly Connected Components', 'Longest common subsequence'])\n",
      "('https://mitpress.mit.edu/books/introduction-algorithms\\n', ['Dfs', 'Shortest Path', 'Randomized algorithms', 'Shortest Paths', 'Heaps', 'Scheduling', 'Topological Sort', 'Huffman codes', 'Strongly Connected Components', 'Longest common subsequence'])\n",
      "('https://www.coursera.org/specializations/algorithms\\n', ['Dfs', 'Shortest Path', 'Randomized algorithms', 'Shortest Paths', 'Heaps', 'Scheduling', 'Topological Sort', 'Huffman codes', 'Strongly Connected Components', 'Longest common subsequence'])\n",
      "('https://en.wikipedia.org/wiki/Breadth-first_search\\n', ['Dfs', 'Shortest Path', 'Randomized algorithms', 'Shortest Paths', 'Heaps', 'Scheduling', 'Topological Sort', 'Huffman codes', 'Strongly Connected Components', 'Longest common subsequence'])\n",
      "('https://en.wikipedia.org/wiki/Grammar_induction\\n', ['Dfs', 'Shortest Path', 'Randomized algorithms', 'Shortest Paths', 'Heaps', 'Scheduling', 'Topological Sort', 'Huffman codes', 'Strongly Connected Components', 'Longest common subsequence'])\n",
      "('https://www.youtube.com/watch?v=tlPuVe5Otio\\n', ['Dfs', 'Shortest Path', 'Randomized algorithms', 'Shortest Paths', 'Topological Sort', 'Heaps', 'Scheduling', 'Huffman codes', 'Strongly Connected Components', 'Longest common subsequence'])\n",
      "('http://www.bogotobogo.com/Algorithms/algorithms.php\\n', ['Dfs', 'Shortest Path', 'Randomized algorithms', 'Shortest Paths', 'Heaps', 'Scheduling', 'Topological Sort', 'Huffman codes', 'Strongly Connected Components', 'Longest common subsequence'])\n",
      "('https://edurev.in/studytube/DESIGN-AND-ANALYSIS-OF-ALGORITHM-Notes,-Computer-Science-Engineering/c8f59288-80ca-4f72-9ed2-7db418b505f7_p\\n', ['Dfs', 'Shortest Path', 'Randomized algorithms', 'Shortest Paths', 'Heaps', 'Scheduling', 'Topological Sort', 'Huffman codes', 'Strongly Connected Components', 'Longest common subsequence'])\n",
      "('https://twitter.com/share?url=https://www.geeksforgeeks.org/greedy-algorithms-set-1-activity-selection-problem/&text=Greedy Algorithms | Set 1 (Activity Selection Problem)&hashtags=GeeksforGeeks\\n', ['Dfs', 'Shortest Path', 'Randomized algorithms', 'Shortest Paths', 'Heaps', 'Scheduling', 'Topological Sort', 'Huffman codes', 'Strongly Connected Components', 'Longest common subsequence'])\n",
      "('https://brilliant.org/wiki/depth-first-search-dfs/\\n', ['Dfs', 'Shortest Path', 'Randomized algorithms', 'Shortest Paths', 'Scheduling', 'Topological Sort', 'Heaps', 'Huffman codes', 'Strongly Connected Components', 'Longest common subsequence'])\n",
      "('http://algo.ics.hawaii.edu/~nodari/teaching/f15/index.html\\n', ['Dfs', 'Shortest Path', 'Randomized algorithms', 'Shortest Paths', 'Heaps', 'Scheduling', 'Topological Sort', 'Huffman codes', 'Strongly Connected Components', 'Longest common subsequence'])\n",
      "('http://staff.ustc.edu.cn/~csli/graduate/algorithms/book6/chap17.htm\\n', ['Dfs', 'Shortest Path', 'Randomized algorithms', 'Shortest Paths', 'Heaps', 'Scheduling', 'Topological Sort', 'Huffman codes', 'Strongly Connected Components', 'Longest common subsequence'])\n",
      "('https://stackoverflow.com/questions/tagged/breadth-first-search?sort=frequent&pageSize=50\\n', ['Dfs', 'Shortest Path', 'Randomized algorithms', 'Shortest Paths', 'Heaps', 'Scheduling', 'Topological Sort', 'Huffman codes', 'Strongly Connected Components', 'Longest common subsequence'])\n",
      "('http://www.linkedin.com/shareArticle?mini=true&url=https://www.geeksforgeeks.org/greedy-algorithms-set-1-activity-selection-problem/\\n', ['Dfs', 'Shortest Path', 'Randomized algorithms', 'Shortest Paths', 'Heaps', 'Scheduling', 'Topological Sort', 'Huffman codes', 'Strongly Connected Components', 'Longest common subsequence'])\n",
      "('http://faculty.ycp.edu/~dbabcock/PastCourses/cs360/lectures/index.html\\n', ['Dfs', 'Shortest Path', 'Randomized algorithms', 'Shortest Paths', 'Heaps', 'Scheduling', 'Topological Sort', 'Huffman codes', 'Strongly Connected Components', 'Longest common subsequence'])\n",
      "('https://auth.geeksforgeeks.org/?to=https://www.geeksforgeeks.org/greedy-algorithms-set-1-activity-selection-problem/\\n', ['Dfs', 'Shortest Path', 'Randomized algorithms', 'Shortest Paths', 'Heaps', 'Scheduling', 'Topological Sort', 'Huffman codes', 'Strongly Connected Components', 'Longest common subsequence'])\n",
      "('https://en.wikipedia.org/wiki/Post-order_traversal\\n', ['Dfs', 'Shortest Path', 'Randomized algorithms', 'Shortest Paths', 'Heaps', 'Scheduling', 'Topological Sort', 'Huffman codes', 'Strongly Connected Components', 'Longest common subsequence'])\n",
      "('https://www.ics.uci.edu/~eppstein/161/960215.html\\n', ['Dfs', 'Shortest Path', 'Randomized algorithms', 'Shortest Paths', 'Heaps', 'Scheduling', 'Topological Sort', 'Huffman codes', 'Strongly Connected Components', 'Longest common subsequence'])\n",
      "('http://www.cs.toronto.edu/~heap/270F02/node37.html\\n', ['Dfs', 'Shortest Path', 'Randomized algorithms', 'Shortest Paths', 'Heaps', 'Scheduling', 'Topological Sort', 'Huffman codes', 'Strongly Connected Components', 'Longest common subsequence'])\n",
      "('https://www.hackerearth.com/practice/algorithms/greedy/basics-of-greedy-algorithms/\\n', ['Shortest Path', 'Dfs', 'Randomized algorithms', 'Shortest Paths', 'Scheduling', 'Topological Sort', 'Heaps', 'Huffman codes', 'Strongly Connected Components', 'Longest common subsequence'])\n",
      "('https://en.wikipedia.org/wiki/Depth-first_search\\n', ['Dfs', 'Shortest Path', 'Randomized algorithms', 'Shortest Paths', 'Heaps', 'Scheduling', 'Topological Sort', 'Huffman codes', 'Strongly Connected Components', 'Longest common subsequence'])\n",
      "('http://www.slader.com/textbook/9780262033848-introduction-to-algorithms-3rd-edition/\\n', ['Shortest Path', 'Dfs', 'Randomized algorithms', 'Shortest Paths', 'Heaps', 'Scheduling', 'Topological Sort', 'Huffman codes', 'Strongly Connected Components', 'Longest common subsequence'])\n",
      "('https://www.geeksforgeeks.org/greedy-algorithms-set-1-activity-selection-problem/\\n', ['Dfs', 'Shortest Path', 'Randomized algorithms', 'Heaps', 'Shortest Paths', 'Scheduling', 'Topological Sort', 'Huffman codes', 'Strongly Connected Components', 'Longest common subsequence'])\n",
      "('https://en.wikipedia.org/wiki/Exchange_algorithm\\n', ['Dfs', 'Shortest Path', 'Randomized algorithms', 'Shortest Paths', 'Heaps', 'Scheduling', 'Topological Sort', 'Huffman codes', 'Strongly Connected Components', 'Longest common subsequence'])\n",
      "('https://codereview.stackexchange.com/questions/tagged/breadth-first-search\\n', ['Dfs', 'Shortest Path', 'Randomized algorithms', 'Shortest Paths', 'Heaps', 'Scheduling', 'Topological Sort', 'Huffman codes', 'Strongly Connected Components', 'Longest common subsequence'])\n",
      "('http://www3.cs.stonybrook.edu/~algorith/files/topological-sorting.shtml\\n', ['Dfs', 'Shortest Path', 'Randomized algorithms', 'Shortest Paths', 'Heaps', 'Scheduling', 'Topological Sort', 'Huffman codes', 'Strongly Connected Components', 'Longest common subsequence'])\n",
      "('https://en.wikipedia.org/wiki/Sparse_PCA\\n', ['Dfs', 'Shortest Path', 'Randomized algorithms', 'Shortest Paths', 'Heaps', 'Scheduling', 'Topological Sort', 'Huffman codes', 'Strongly Connected Components', 'Longest common subsequence'])\n",
      "('http://www.tumblr.com/share/link?url=https://www.geeksforgeeks.org/greedy-algorithms-set-1-activity-selection-problem/&title=Greedy Algorithms | Set 1 (Activity Selection Problem)\\n', ['Dfs', 'Shortest Path', 'Randomized algorithms', 'Shortest Paths', 'Heaps', 'Scheduling', 'Topological Sort', 'Huffman codes', 'Strongly Connected Components', 'Longest common subsequence'])\n",
      "('https://en.wikipedia.org/wiki/Optimal_substructure', ['Shortest Path', 'Dfs', 'Randomized algorithms', 'Shortest Paths', 'Heaps', 'Scheduling', 'Topological Sort', 'Huffman codes', 'Strongly Connected Components', 'Longest common subsequence'])\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 2, 1):\n",
    "    s, doc_to_concepts_list, m = get_score(content, index, 10, document_term_frequency, dtf_asint, coocc)\n",
    "    for k, v in doc_to_concepts_list.items():\n",
    "        print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_study_clustering_engage.txt\n",
      "('len', 30)\n",
      "('len', 30)\n",
      "('burden str', '& 1.0&0.858')\n",
      "('max str', '& 1.0&0.857')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for root, dirs, files in os.walk(\"lps/engage/\"):  \n",
    "    for filename in files[:1]:\n",
    "        content, document_term_frequency, dtf_asint, coocc, index = get_required(\"lps/engage/\"+filename)\n",
    "        print(filename)\n",
    "        result_str = \"\"\n",
    "        max_str = \"\"\n",
    "        og = 0.0\n",
    "        og_max = 0.0\n",
    "        for i in range(1, 3, 1):\n",
    "            s, doc_to_concepts_list, max_burden_doc = get_score(content, index, i, document_term_frequency, dtf_asint, coocc)\n",
    "            if(i == 1):\n",
    "                result_str += \"& \"+str(1.0)\n",
    "                og = s\n",
    "                max_str += \"& \"+str(1.0)\n",
    "                og_max = max_burden_doc\n",
    "            else:\n",
    "                if(og == 0): print(s)\n",
    "                else:\n",
    "                    result_str += \"&\"+\"{0:.3f}\".format(s/og)\n",
    "                    max_str += \"&\"+\"{0:.3f}\".format(max_burden_doc/og_max)\n",
    "        print(\"burden str\", result_str)\n",
    "        print(\"max str\", max_str)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.5"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "25.266666666666666\n",
    "24.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
